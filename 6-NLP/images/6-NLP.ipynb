{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "6 Natural Language Processing (NLP)\n",
    "---\n",
    "\n",
    "# 6.1 Introduction\n",
    "Where we learn about Elisa, Turing Test, and Computational Lingustics, then build a (random) conversational bot. \n",
    "\n",
    "## 6.1.1 The plan\n",
    "\n",
    "Your steps when building a conversational bot:\n",
    "\n",
    "1. Print instructions advising the user how to interact with the bot\n",
    "2. Start a loop\n",
    "    * Accept user input\n",
    "    * If user has asked to exit, then exit\n",
    "    * Process user input and determine response (in this case, the response is a random choice from a list of possible generic responses)\n",
    "    * Print response\n",
    "3. loop back to step 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Marvin, the simple robot.\n",
      "You can end this conversation at any time by typing 'bye'\n",
      "After typing each answer, press 'enter'\n",
      "How are you today?\n",
      "Why do you say that?\n",
      "It was nice talking to you, goodbye!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# This list contains the random responses (you can add your own or translate them into your own language too)\n",
    "random_responses = [\"That is quite interesting, please tell me more.\",\n",
    "                    \"I see. Do go on.\",\n",
    "                    \"Why do you say that?\",\n",
    "                    \"Funny weather we've been having, isn't it?\",\n",
    "                    \"Let's change the subject.\",\n",
    "                    \"Did you catch the game last night?\"]\n",
    "\n",
    "print(\"Hello, I am Marvin, the simple robot.\")\n",
    "print(\"You can end this conversation at any time by typing 'bye'\")\n",
    "print(\"After typing each answer, press 'enter'\")\n",
    "print(\"How are you today?\")\n",
    "\n",
    "while True:\n",
    "    # wait for the user to enter some text\n",
    "    user_input = input(\"> \")\n",
    "    if user_input.lower() == \"bye\":\n",
    "        # if they typed in 'bye' (or even BYE, ByE, byE etc.), break out of the loop\n",
    "        break\n",
    "    else:\n",
    "        response = random.choices(random_responses)[0]\n",
    "    print(response)\n",
    "\n",
    "print(\"It was nice talking to you, goodbye!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Stop and consider\n",
    "\n",
    "* Do you think the random responses would 'trick' someone into thinking that the bot actually understood them?\n",
    "* What features would the bot need to be more effective?\n",
    "* If a bot could really 'understand' the meaning of a sentence, would it need to 'remember' the meaning of previous sentences in a conversation too?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 [Common NLP tasks](https://paperswithcode.com/area/natural-language-processing) and techniques\n",
    "Where we learn about **tokenization** (splitting text into *tokens*), Embeddings (convert text to numerical data so that similar / related words cluster), Parsing & Part of speech tagging (e.g., as a noun, verb, or adjective), word and phrase frequencies, n-grams (A text can be split into sequences of words of a set length $n$), Noun phrase Extraction, [Sentiment analysis](https://paperswithcode.com/task/sentiment-analysis), inflection, and lemmatization (rooting of words). There are useful databases like WordNet, [GLUE and SST](https://paperswithcode.com/datasets?task=sentiment-analysis) and frameworks from classical NLP (e.g., TextBlob) or Deep Learning (e.g., [Transormers](https://huggingface.co/learn/nlp-course)). \n",
    "\n",
    "üí°Take a look at a [deep learning embeddings on TensorFlow (and recall PCA and T-SNE)](https://projector.tensorflow.org)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Exercise  - using `TextBlob` library\n",
    "\n",
    "Let's use a library called TextBlob as it contains helpful APIs for tackling these types of tasks. TextBlob \"stands on the giant shoulders of [NLTK](https://nltk.org) and [pattern](https://github.com/clips/pattern), and plays nicely with both.\" It has a considerable amount of ML embedded in its API.\n",
    "\n",
    "> Note: A useful [Quick Start](https://textblob.readthedocs.io/en/dev/quickstart.html#quickstart) guide is available for TextBlob that is recommended for experienced Python developers\n",
    "\n",
    "When attempting to identify *noun phrases*, TextBlob offers several options of extractors to find noun phrases.\n",
    "\n",
    "1. Take a look at `ConllExtractor`.\n",
    "\n",
    "   ```python\n",
    "   from textblob import TextBlob\n",
    "   from textblob.np_extractors import ConllExtractor\n",
    "   # import and create a Conll extractor to use later \n",
    "   extractor = ConllExtractor()\n",
    "\n",
    "   # later when you need a noun phrase extractor:\n",
    "   user_input = input(\"> \")\n",
    "   user_input_blob = TextBlob(user_input, np_extractor=extractor)  # note non-default extractor specified\n",
    "   np = user_input_blob.noun_phrases                                  \n",
    "   ```\n",
    "   > What's going on here? [ConllExtractor](https://textblob.readthedocs.io/en/dev/api_reference.html?highlight=Conll#textblob.en.np_extractors.ConllExtractor) is \"A noun phrase extractor that uses chunk parsing trained with the ConLL-2000 training corpus.\" ConLL-2000 refers to the 2000 Conference on Computational Natural Language Learning. Each year the conference hosted a workshop to tackle a thorny NLP problem, and in 2000 it was noun chunking. A model was trained on the Wall Street Journal, with \"sections 15-18 as training data (211727 tokens) and section 20 as test data (47377 tokens)\". You can look at the procedures used [here](https://www.clips.uantwerpen.be/conll2000/chunking/) and the [results](https://ifarm.nl/erikt/research/np-chunking.html).\n",
    "   >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Challenge - improving our bot with NLP\n",
    "\n",
    "In 6.1 we built a very simple Q&A bot. Now, we'll make Marvin a bit more sympathetic by analyzing your input for sentiment and printing out a response to match the sentiment. We'll also need to identify a `noun_phrase` and ask about it.\n",
    "\n",
    "Our steps when building a better conversational bot:\n",
    "\n",
    "1. Print instructions advising the user how to interact with the bot\n",
    "2. Start loop\n",
    "   1. Accept user input\n",
    "   2. If user has asked to exit, then exit\n",
    "   3. Process user input and determine appropriate sentiment response\n",
    "   4. If a noun phrase is detected in the sentiment, pluralize it and ask for more input on that topic\n",
    "   5. Print response\n",
    "3. loop back to step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to /Users/cer/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Marvin, the simple robot.\n",
      "You can end this conversation at any time by typing 'bye'\n",
      "After typing each answer, press 'enter'\n",
      "How are you today?\n",
      "Hmm, that's not great. Can you tell me more?\n",
      "It was nice talking to you, goodbye!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from textblob import TextBlob\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "extractor = ConllExtractor()\n",
    "\n",
    "  \n",
    "nltk.download('conll2000')\n",
    "\n",
    "print(\"Hello, I am Marvin, the simple robot.\")\n",
    "print(\"You can end this conversation at any time by typing 'bye'\")\n",
    "print(\"After typing each answer, press 'enter'\")\n",
    "print(\"How are you today?\")\n",
    "\n",
    "while True:\n",
    "        # wait for the user to enter some text\n",
    "        user_input = input(\"> \")\n",
    "\n",
    "        if user_input.lower() == \"bye\":            \n",
    "            # if they typed in 'bye' (or even BYE, ByE, byE etc.), break out of the loop\n",
    "            break\n",
    "        else:\n",
    "            # Create a TextBlob based on the user input. Then extract the noun phrases\n",
    "            user_input_blob = TextBlob(user_input, np_extractor=extractor)                        \n",
    "            np = user_input_blob.noun_phrases                                    \n",
    "            response = \"\"\n",
    "            if user_input_blob.polarity <= -0.5:\n",
    "                response = \"Oh dear, that sounds bad. \"\n",
    "            elif user_input_blob.polarity <= 0:\n",
    "                response = \"Hmm, that's not great. \"\n",
    "            elif user_input_blob.polarity <= 0.5:\n",
    "                response = \"Well, that sounds positive. \"\n",
    "            elif user_input_blob.polarity <= 1:\n",
    "                response = \"Wow, that sounds great. \"\n",
    "\n",
    "            if len(np) != 0:\n",
    "                # There was at least one noun phrase detected, so ask about that and pluralise it\n",
    "                # e.g. cat -> cats or mouse -> mice\n",
    "                response = response + \"Can you tell me more about \" + np[0].pluralize() + \"?\"\n",
    "            else:\n",
    "                response = response + \"Can you tell me more?\"\n",
    "            print(response)\n",
    "    \n",
    "print(\"It was nice talking to you, goodbye!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Translation and sentiment analysis with ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Det er en sandhed, der universelt er anerkendt, at en enkelt mand, der er i besiddelse af en lykke, skal v√¶re i mangel p√• en kone!\")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(\n",
    "    \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife!\"\n",
    ")\n",
    "blob.translate(from_lang=\"en\",to=\"da\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What's going on here? and why is TextBlob so good at translation? Well, behind the scenes, it's using Google translate, a sophisticated AI able to parse millions of phrases to predict the best strings for the task at hand. There's nothing manual going on here and you need an internet connection to use `blob.translate`.\n",
    "\n",
    "‚úÖ Try some more sentences. Which is better, ML or human translation? In which cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Sentiment \n",
    "Sentiment is measured in with a *polarity* of -1 to 1, meaning -1 is the most negative sentiment, and 1 is the most positive. \n",
    "\n",
    "Sentiment is also measured with an 0 - 1 score for objectivity (0) and subjectivity (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. has a sentiment of Sentiment(polarity=0.20952380952380953, subjectivity=0.27142857142857146)\n",
      "Darcy, as well as Elizabeth, really loved them; and they were both ever sensible of the warmest gratitude towards the persons who, by bringing her into Derbyshire, had been the means of uniting them. has a sentiment of Sentiment(polarity=0.7, subjectivity=0.8)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "quote1 = \"\"\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\"\"\"\n",
    "\n",
    "quote2 = \"\"\"Darcy, as well as Elizabeth, really loved them; and they were both ever sensible of the warmest gratitude towards the persons who, by bringing her into Derbyshire, had been the means of uniting them.\"\"\"\n",
    "\n",
    "sentiment1 = TextBlob(quote1).sentiment\n",
    "sentiment2 = TextBlob(quote2).sentiment\n",
    "\n",
    "print(quote1 + \" has a sentiment of \" + str(sentiment1))\n",
    "print(quote2 + \" has a sentiment of \" + str(sentiment2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge - check sentiment polarity\n",
    "Our task is to determine, using sentiment polarity, if *Pride and Prejudice* has more absolutely positive sentences than absolutely negative ones. For this task, you may assume that a polarity score of 1 or -1 is absolutely positive or negative respectively.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. ~~Download a [copy of Pride and Prejudice](https://www.gutenberg.org/files/1342/1342-h/1342-h.htm) from Project Gutenberg as a .txt file. Remove the metadata at the start and end of the file, leaving only the original text~~ Cumhur used `curl https://raw.githubusercontent.com/cs109/2015lab8/master/data/pride_and_prejudice.txt -o pride.txt` and did not clean up (as metadata is neutral). \n",
    "2. Open the file in Python and extract the contents as a string\n",
    "3. Create a TextBlob using the book string\n",
    "4. Analyse each sentence in the book in a loop\n",
    "   1. If the polarity is 1 or -1 store the sentence in an array or list of positive or negative messages\n",
    "5. At the end, print out all the positive sentences and negative sentences (separately) and the number of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should download the book text, clean it, and import it here\n",
    "with open(\"pride.txt\", encoding=\"utf8\") as f:\n",
    "    file_contents = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_pride = TextBlob(file_contents)\n",
    "positive_sentiment_sentences = []\n",
    "negative_sentiment_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in book_pride.sentences:\n",
    "    if sentence.sentiment.polarity == 1:\n",
    "        positive_sentiment_sentences.append(sentence)\n",
    "    if sentence.sentiment.polarity == -1:\n",
    "        negative_sentiment_sentences.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 39 most positive sentences:\n",
      "+ \"What an excellent father you have, girls!\"\n",
      "+ Everybody said how wellshe looked; and Mr. Bingley thought her quite beautiful, and danced withher twice!\n",
      "+ He walked here, and he walked there, fancying himself so verygreat!\n",
      "+ Elizabeth assured him that she could suit herself perfectly with thosein the room.\n",
      "+ What a delightful library you have atPemberley, Mr.\n",
      "+ Her performance on the pianoforte is exquisite.\"\n",
      "+ yes--I understand you perfectly.\"\n",
      "+ \"I am perfectly convinced by it that Mr. Darcy has no defect.\n",
      "+ \"It _is_ wonderful,\" replied Wickham, \"for almost all his actions maybe traced to pride; and pride had often been his best friend.\n",
      "+ Family pride, and _filial_ pride--for he is very proud of whathis father was--have done this.\n",
      "+ _That_ would be the greatest misfortune of all!\n",
      "+ How wonderfully these sort of things occur!\n",
      "+ She owedher greatest relief to her friend Miss Lucas, who often joined them, andgood-naturedly engaged Mr. Collins's conversation to herself.\n",
      "+ \"An excellent consolation in its way,\" said Elizabeth, \"but it will notdo for _us_.\n",
      "+ The improvementof spending a night in London was added in time, and the plan becameperfect as plan could be.\n",
      "+ It is the greatest of favourswhen Miss de Bourgh comes in.\"\n",
      "+ Anne would havebeen a delightful performer, had her health allowed her to learn.\"\n",
      "+ \"Perfectly so, I thank you.\"\n",
      "+ She is avery great favourite with some ladies of my acquaintance, Mrs. Hurst andMiss Bingley.\n",
      "+ Perhaps thisconcealment, this disguise was beneath me; it is done, however, and itwas done for the best.\n",
      "+ I have the greatest dislike inthe world to that sort of thing.\n",
      "+ cried Elizabeth, with the greatest satisfaction.\n",
      "+ Charlotte is anexcellent manager, I dare say.\n",
      "+ \"His father was an excellent man,\" said Mrs. Gardiner.\n",
      "+ \"He is perfectly well behaved, polite, and unassuming,\" said her uncle.\n",
      "+ On reaching the house, they were shown through the hall into the saloon,whose northern aspect rendered it delightful for summer.\n",
      "+ Ourdistress, my dear Lizzy, is very great.\n",
      "+ And tell my dear Lydia not togive any directions about her clothes till she has seen me, for she doesnot know which are the best warehouses.\n",
      "+ We acted with the best intentions.\"\n",
      "+ It now occurred to the girls that their mother was in all likelihoodperfectly ignorant of what had happened.\n",
      "+ \"This is delightful indeed!\n",
      "+ I am so happy!\n",
      "+ But, however, he is very welcometo come to Netherfield, if he likes it.\n",
      "+ Happy shall I be, when his stay at Netherfield is over!\"\n",
      "+ You will be a very happy woman.\"\n",
      "+ my dear, dear Jane, I am sohappy!\n",
      "+ If I could but see _you_ as happy!\n",
      "+ He is perfectly amiable.\n",
      "+ Youridea of the ponies is delightful.\n"
     ]
    }
   ],
   "source": [
    "print(\"The \" + str(len(positive_sentiment_sentences)) + \" most positive sentences:\")\n",
    "for sentence in positive_sentiment_sentences:\n",
    "    print(\"+ \" + str(sentence.replace(\"\\n\", \"\").replace(\"      \", \" \")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 18 most negative sentences:\n",
      "- shocking!\"\n",
      "- Everybody is disgusted with his pride.\n",
      "- \"This is quite shocking!\n",
      "- What canhave induced him to behave so cruelly?\"\n",
      "- His dispositionmust be dreadful.\"\n",
      "- To finda man agreeable whom one is determined to hate!\n",
      "- \"You shall hear then--but prepare yourself for something very dreadful.\n",
      "- The pause was to Elizabeth's feelingsdreadful.\n",
      "- \"Wickham sovery bad!\n",
      "- The separationbetween her and her family was rather noisy than pathetic.\n",
      "- It would be dreadful!\n",
      "- It is every way horrible!\"\n",
      "- \"Oh, yes!--that, that is the worst of all.\n",
      "- \"She is so fond of Mrs. Forster,\" said she, \"it will be quite shockingto send her away!\n",
      "- It was all over before I arrived; so my curiosity was not sodreadfully racked as _yours_ seems to have been.\n",
      "- Hecalled it, therefore, his duty to step forward, and endeavour to remedyan evil which had been brought on by himself.\n",
      "- \"Hate you!\n",
      "- You were disgusted with the women who were always speaking,and looking, and thinking for _your_ approbation alone.\n"
     ]
    }
   ],
   "source": [
    "print(\"The \" + str(len(negative_sentiment_sentences)) + \" most negative sentences:\")\n",
    "for sentence in negative_sentiment_sentences:\n",
    "    print(\"- \" + str(sentence.replace(\"\\n\", \"\").replace(\"      \", \" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Knowledge Check: Consult to the [3-Translation-Sentiment/README.md](../3-Translation-Sentiment/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Hotel Reviews 1\n",
    "\n",
    "See [4-Hotel-Reviews-1/README.md](../4-Hotel-Reviews-1/README.md) for details and the questions we ask. \n",
    "\n",
    "‚ùóÔ∏èIf you don't like downloading from Kaggle or elsewhere as we do, you coudl try if you have huggingface datasets installed:\n",
    " ```python\n",
    " from datasets import load_dataset\n",
    " dataset = load_dataset(\"ashraq/hotel-reviews\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDA\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_difference_review_avg(row):\n",
    "    return row[\"Average_Score\"] - row[\"Calc_Average_Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file now, this could take a while depending on file size\n",
      "Loading took 1.88 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the hotel reviews from CSV\n",
    "print(\"Loading data file now, this could take a while depending on file size\")\n",
    "start = time.time()\n",
    "df = pd.read_csv('Hotel_Reviews.csv')\n",
    "end = time.time()\n",
    "print(\"Loading took \" + str(round(end - start, 2)) + \" seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data (rows, cols) is (515738, 17)\n"
     ]
    }
   ],
   "source": [
    "# What shape is the data (rows, columns)?\n",
    "print(\"The shape of the data (rows, cols) is \" + str(df.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_counts() creates a Series object that has index and values\n",
    "#                in this case, the country and the frequency they occur in reviewer nationality\n",
    "nationality_freq = df[\"Reviewer_Nationality\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest frequency reviewer nationality is United Kingdom with 245246 reviews.\n"
     ]
    }
   ],
   "source": [
    "# What reviewer nationality is the most common in the dataset?\n",
    "print(\"The highest frequency reviewer nationality is \" + str(nationality_freq.index[0]).strip() + \" with \" + str(nationality_freq[0]) + \" reviews.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 highest frequency reviewer nationalities are:\n",
      " United Kingdom               245246\n",
      " United States of America      35437\n",
      " Australia                     21686\n",
      " Ireland                       14827\n",
      " United Arab Emirates          10235\n",
      " Saudi Arabia                   8951\n",
      " Netherlands                    8772\n",
      " Switzerland                    8678\n",
      " Germany                        7941\n",
      " Canada                         7894\n"
     ]
    }
   ],
   "source": [
    "# What is the top 10 most common nationalities and their frequencies?\n",
    "print(\"The top 10 highest frequency reviewer nationalities are:\")\n",
    "print(nationality_freq[0:10].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 227 unique nationalities in the dataset\n"
     ]
    }
   ],
   "source": [
    "# How many unique nationalities are there?\n",
    "print(\"There are \" + str(nationality_freq.index.size) + \" unique nationalities in the dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most reviewed hotel for United Kingdom was Britannia International Hotel Canary Wharf with 3833 reviews.\n",
      "The most reviewed hotel for United States of America was Hotel Esther a with 423 reviews.\n",
      "The most reviewed hotel for Australia was Park Plaza Westminster Bridge London with 167 reviews.\n",
      "The most reviewed hotel for Ireland was Copthorne Tara Hotel London Kensington with 239 reviews.\n",
      "The most reviewed hotel for United Arab Emirates was Millennium Hotel London Knightsbridge with 129 reviews.\n",
      "The most reviewed hotel for Saudi Arabia was The Cumberland A Guoman Hotel with 142 reviews.\n",
      "The most reviewed hotel for Netherlands was Jaz Amsterdam with 97 reviews.\n",
      "The most reviewed hotel for Switzerland was Hotel Da Vinci with 97 reviews.\n",
      "The most reviewed hotel for Germany was Hotel Da Vinci with 86 reviews.\n",
      "The most reviewed hotel for Canada was St James Court A Taj Hotel London with 61 reviews.\n"
     ]
    }
   ],
   "source": [
    "# What was the most frequently reviewed hotel for the top 10 nationalities - print the hotel and number of reviews\n",
    "for nat in nationality_freq[:10].index:\n",
    "   # First, extract all the rows that match the criteria into a new dataframe\n",
    "   nat_df = df[df[\"Reviewer_Nationality\"] == nat]   \n",
    "   # Now get the hotel freq\n",
    "   freq = nat_df[\"Hotel_Name\"].value_counts()\n",
    "   print(\"The most reviewed hotel for \" + str(nat).strip() + \" was \" + str(freq.index[0]) + \" with \" + str(freq[0]) + \" reviews.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many reviews are there per hotel (frequency count of hotel) and do the results match the value in `Total_Number_of_Reviews`?\n",
    "# First create a new dataframe based on the old one, removing the uneeded columns\n",
    "hotel_freq_df = df.drop([\"Hotel_Address\", \"Additional_Number_of_Scoring\", \"Review_Date\", \"Average_Score\", \"Reviewer_Nationality\", \"Negative_Review\", \"Review_Total_Negative_Word_Counts\", \"Positive_Review\", \"Review_Total_Positive_Word_Counts\", \"Total_Number_of_Reviews_Reviewer_Has_Given\", \"Reviewer_Score\", \"Tags\", \"days_since_review\", \"lat\", \"lng\"], axis = 1)\n",
    "# Group the rows by Hotel_Name, count them and put the result in a new column Total_Reviews_Found\n",
    "hotel_freq_df['Total_Reviews_Found'] = hotel_freq_df.groupby('Hotel_Name').transform('count')\n",
    "# Get rid of all the duplicated rows\n",
    "hotel_freq_df = hotel_freq_df.drop_duplicates(subset = [\"Hotel_Name\"])\n",
    "# print()\n",
    "# print(hotel_freq_df.to_string())\n",
    "# print(str(hotel_freq_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Average_Score_Difference  Average_Score  Calc_Average_Score  \\\n",
      "495945                      -0.8            7.7                 8.5   \n",
      "111027                      -0.7            8.8                 9.5   \n",
      "43688                       -0.7            7.5                 8.2   \n",
      "178253                      -0.7            7.9                 8.6   \n",
      "218258                      -0.5            7.0                 7.5   \n",
      "...                          ...            ...                 ...   \n",
      "151416                       0.7            7.8                 7.1   \n",
      "22189                        0.8            7.1                 6.3   \n",
      "250308                       0.9            8.6                 7.7   \n",
      "68936                        0.9            6.8                 5.9   \n",
      "3813                         1.3            7.2                 5.9   \n",
      "\n",
      "                                               Hotel_Name  \n",
      "495945                         Best Western Hotel Astoria  \n",
      "111027  Hotel Stendhal Place Vend me Paris MGallery by...  \n",
      "43688                       Mercure Paris Porte d Orleans  \n",
      "178253                    Renaissance Paris Vendome Hotel  \n",
      "218258                                Hotel Royal Elys es  \n",
      "...                                                   ...  \n",
      "151416                        Best Western Allegro Nation  \n",
      "22189              Holiday Inn Paris Montparnasse Pasteur  \n",
      "250308          MARQUIS Faubourg St Honor Relais Ch teaux  \n",
      "68936                                       Villa Eugenie  \n",
      "3813                                   Kube Hotel Ice Bar  \n",
      "\n",
      "[1492 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# While there is an `Average_Score` for each hotel according to the dataset, \n",
    "# you can also calculate an average score (getting the average of all reviewer scores in the dataset for each hotel)\n",
    "# Add a new column to your dataframe with the column header `Calc_Average_Score` that contains that calculated average. \n",
    "df['Calc_Average_Score'] = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)\n",
    "# Add a new column with the difference between the two average scores\n",
    "df[\"Average_Score_Difference\"] = df.apply(get_difference_review_avg, axis = 1)\n",
    "# Create a df without all the duplicates of Hotel_Name (so only 1 row per hotel)\n",
    "review_scores_df = df.drop_duplicates(subset = [\"Hotel_Name\"])\n",
    "# Sort the dataframe to find the lowest and highest average score difference\n",
    "review_scores_df = review_scores_df.sort_values(by=[\"Average_Score_Difference\"])\n",
    "print(review_scores_df[[\"Average_Score_Difference\", \"Average_Score\", \"Calc_Average_Score\", \"Hotel_Name\"]])\n",
    "# Do any hotels have the same (rounded to 1 decimal place) `Average_Score` and `Calc_Average_Score`?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí°As we proceed to NLP, take this opportunity to read through the '[NLTK book](https://www.nltk.org/book/)' and try out its exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5-Hotel-Reviews-2: Sentiment Analysis\n",
    "\n",
    "Now that you have explored the dataset in detail, it's time to filter the columns and then use NLP techniques on the dataset to gain new insights about the hotels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: a bit more data processing\n",
    "\n",
    "Clean the data just a bit more. Add columns that will be useful later, change the values in other columns, and drop certain columns completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_address(row):\n",
    "    if \"Netherlands\" in row[\"Hotel_Address\"]:\n",
    "        return \"Amsterdam, Netherlands\"\n",
    "    elif \"Barcelona\" in row[\"Hotel_Address\"]:\n",
    "        return \"Barcelona, Spain\"\n",
    "    elif \"United Kingdom\" in row[\"Hotel_Address\"]:\n",
    "        return \"London, United Kingdom\"\n",
    "    elif \"Milan\" in row[\"Hotel_Address\"]:        \n",
    "        return \"Milan, Italy\"\n",
    "    elif \"France\" in row[\"Hotel_Address\"]:\n",
    "        return \"Paris, France\"\n",
    "    elif \"Vienna\" in row[\"Hotel_Address\"]:\n",
    "        return \"Vienna, Austria\" \n",
    "    else:\n",
    "        return row.Hotel_Address\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns we will not use:\n",
    "df.drop([\"lat\", \"lng\"], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all the addresses with a shortened, more useful form\n",
    "df[\"Hotel_Address\"] = df.apply(replace_address, axis = 1)\n",
    "# Drop `Additional_Number_of_Scoring`\n",
    "df.drop([\"Additional_Number_of_Scoring\"], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace `Total_Number_of_Reviews` and `Average_Score` with our own calculated values\n",
    "df.Average_Score = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the Tags into new columns\n",
    "# The file Hotel_Reviews_Tags.py, identifies the most important tags\n",
    "# Leisure trip, Couple, Solo traveler, Business trip, Group combined with Travelers with friends, \n",
    "# Family with young children, Family with older children, With a pet\n",
    "df[\"Leisure_trip\"] = df.Tags.apply(lambda tag: 1 if \"Leisure trip\" in tag else 0)\n",
    "df[\"Couple\"] = df.Tags.apply(lambda tag: 1 if \"Couple\" in tag else 0)\n",
    "df[\"Solo_traveler\"] = df.Tags.apply(lambda tag: 1 if \"Solo traveler\" in tag else 0)\n",
    "df[\"Business_trip\"] = df.Tags.apply(lambda tag: 1 if \"Business trip\" in tag else 0)\n",
    "df[\"Group\"] = df.Tags.apply(lambda tag: 1 if \"Group\" in tag or \"Travelers with friends\" in tag else 0)\n",
    "df[\"Family_with_young_children\"] = df.Tags.apply(lambda tag: 1 if \"Family with young children\" in tag else 0)\n",
    "df[\"Family_with_older_children\"] = df.Tags.apply(lambda tag: 1 if \"Family with older children\" in tag else 0)\n",
    "df[\"With_a_pet\"] = df.Tags.apply(lambda tag: 1 if \"With a pet\" in tag else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No longer need any of these columns\n",
    "df.drop([\"Review_Date\", \"Review_Total_Negative_Word_Counts\", \"Review_Total_Positive_Word_Counts\", \"days_since_review\", \"Total_Number_of_Reviews_Reviewer_Has_Given\"], axis = 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to Hotel_Reviews_Filtered.csv\n",
      "Filtering took 1719.01 seconds\n"
     ]
    }
   ],
   "source": [
    "# Saving new data file with calculated columns\n",
    "print(\"Saving results to Hotel_Reviews_Filtered.csv\")\n",
    "df.to_csv(r'Hotel_Reviews_Filtered.csv', index = False)\n",
    "end = time.time()\n",
    "print(\"Filtering took \" + str(round(end - start, 2)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: load and save the filtered data for sentiment analysis\n",
    "The code skeleton should like this (assuming) independent execution:\n",
    "```python\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Load the filtered hotel reviews from CSV\n",
    "df = pd.read_csv('Hotel_Reviews_Filtered.csv')\n",
    "\n",
    "# You code will be added here\n",
    "\n",
    "\n",
    "# Finally remember to save the hotel reviews with new NLP data added\n",
    "print(\"Saving results to Hotel_Reviews_NLP.csv\")\n",
    "df.to_csv(r'Hotel_Reviews_NLP.csv', index = False)\n",
    "```\n",
    "\n",
    "The actual code is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/cer/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/cer/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_sentiment = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 3 possibilities of input for a review:\n",
    "# It could be \"No Negative\", in which case, return 0\n",
    "# It could be \"No Positive\", in which case, return 0\n",
    "# It could be a review, in which case calculate the sentiment\n",
    "def calc_sentiment(review):    \n",
    "    if review == \"No Negative\" or review == \"No Positive\":\n",
    "        return 0\n",
    "    return vader_sentiment.polarity_scores(review)[\"compound\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the hotel reviews from CSV (already filtered)\n",
    "df = pd.read_csv(\"Hotel_Reviews_Filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words - can be slow for a lot of text!\n",
    "# Ryan Han (ryanxjhan on Kaggle) has a great post measuring performance of different stop words removal approaches\n",
    "# https://www.kaggle.com/ryanxjhan/fast-stop-words-removal # using the approach that Ryan recommends\n",
    "start = time.time()\n",
    "cache = set(stopwords.words(\"english\"))\n",
    "def remove_stopwords(review):\n",
    "    text = \" \".join([word for word in review.split() if word not in cache])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the stop words from both columns\n",
    "df.Negative_Review = df.Negative_Review.apply(remove_stopwords)   \n",
    "df.Positive_Review = df.Positive_Review.apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stop words took 26.14 seconds\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Removing stop words took \" + str(round(end - start, 2)) + \" seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating sentiment columns for both positive and negative reviews\n",
      "Calculating sentiment took 68.7 seconds\n"
     ]
    }
   ],
   "source": [
    "# Add a negative sentiment and positive sentiment column. (This can take a moment, ~1,5 minute)\n",
    "print(\"Calculating sentiment columns for both positive and negative reviews\")\n",
    "start = time.time()\n",
    "df[\"Negative_Sentiment\"] = df.Negative_Review.apply(calc_sentiment)\n",
    "df[\"Positive_Sentiment\"] = df.Positive_Review.apply(calc_sentiment)\n",
    "end = time.time()\n",
    "print(\"Calculating sentiment took \" + str(round(end - start, 2)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Negative_Review  Negative_Sentiment\n",
      "186584  So bad experience memories I hotel The first n...             -0.9920\n",
      "129503  First charged twice room booked booking second...             -0.9896\n",
      "307286  The staff Had bad experience even booking Janu...             -0.9889\n",
      "201953  Everything DO NOT STAY AT THIS HOTEL I never i...             -0.9886\n",
      "452092  No WLAN room Incredibly rude restaurant staff ...             -0.9884\n",
      "...                                                   ...                 ...\n",
      "138365  Wifi terribly slow I speed test network upload...              0.9938\n",
      "79215   I find anything hotel first I walked past hote...              0.9938\n",
      "278506  The property great location There bakery next ...              0.9945\n",
      "339189  Guys I like hotel I wish return next year Howe...              0.9948\n",
      "480509  I travel lot far visited countless number hote...              0.9957\n",
      "\n",
      "[515738 rows x 2 columns]\n",
      "                                          Positive_Review  Positive_Sentiment\n",
      "137893  Bathroom Shower We going stay twice hotel 2 ni...             -0.9820\n",
      "5839    I completely disappointed mad since reception ...             -0.9780\n",
      "64158   get everything extra internet parking breakfas...             -0.9751\n",
      "124178  I didnt like anythig Room small Asked upgrade ...             -0.9721\n",
      "489137  Very rude manager abusive staff reception Dirt...             -0.9703\n",
      "...                                                   ...                 ...\n",
      "417442  We celebrated wedding night Langham I commend ...              0.9985\n",
      "322920  From moment stepped doors Guesthouse Hotel sta...              0.9985\n",
      "132492  We arrived super cute boutique hotel area expl...              0.9987\n",
      "287419  When first arrived hotel staff incredibly frie...              0.9987\n",
      "179007  We went Andaz 40th birthday celebration This a...              0.9991\n",
      "\n",
      "[515738 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(by=[\"Negative_Sentiment\"], ascending=True)\n",
    "print(df[[\"Negative_Review\", \"Negative_Sentiment\"]])\n",
    "df = df.sort_values(by=[\"Positive_Sentiment\"], ascending=True)\n",
    "print(df[[\"Positive_Review\", \"Positive_Sentiment\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the columns (This is cosmetic, but to make it easier to explore the data later)\n",
    "df = df.reindex([\"Hotel_Name\", \"Hotel_Address\", \"Total_Number_of_Reviews\", \"Average_Score\", \"Reviewer_Score\", \"Negative_Sentiment\", \"Positive_Sentiment\", \"Reviewer_Nationality\", \"Leisure_trip\", \"Couple\", \"Solo_traveler\", \"Business_trip\", \"Group\", \"Family_with_young_children\", \"Family_with_older_children\", \"With_a_pet\", \"Negative_Review\", \"Positive_Review\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to Hotel_Reviews_NLP.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving results to Hotel_Reviews_NLP.csv\")\n",
    "df.to_csv(r\"Hotel_Reviews_NLP.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLME-23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
